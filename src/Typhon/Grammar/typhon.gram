# PEG grammar for Typhon
# Based on https://github.com/we-like-parsers/pegen/blob/main/data/python.gram

@class TyphonParser

@subheader'''
import enum
import io
import itertools
import os
import token
from typing import (
    Callable, Iterator, List, Literal, NoReturn, Sequence, Tuple, TypeVar, Union
)

from .typhon_ast import (
    assign_as_declaration, make_function_literal, make_arrow_type, make_with_stmt,
    declaration_as_withitem, make_for_stmt, make_function_def, make_comprehension,
    set_is_typing_expression, maybe_optional, set_is_coalescing, set_type_annotation,
    get_optional_question_node, get_force_unwrap_node, make_if_let, make_while_let,
    make_pipe_call, make_arrow_type_single_chain, if_comp_exp,
    make_genexp, make_listcomp, make_setcomp, make_dictcomp,
    make_with_comp, make_try_comp, make_try_comp_except, make_match_comp,
    make_match_comp_case, make_while_comp, make_if_let_comp,
    make_while_let_comp, make_let_comp,
    make_record_literal, make_record_type, make_attributes_pattern, make_tuple_pattern, make_for_let_pattern, make_with_let_pattern, make_inline_with_let_pattern,
)
from .parser_helper import Parser, Load, Store, Del, Target
from .tokenizer_custom import TokenizerCustom
from .token_factory_custom import token_stream_factory

def parse(
    filename: str,
    tokenizer: TokenizerCustom,
    mode: Union[Literal["eval"], Literal["exec"], Literal["file"]],
    py_version: Optional[tuple[int, int]]=None,
    verbose:bool = False,
) -> ast.AST | None:
    parser = TyphonParser(tokenizer, verbose=verbose, py_version=py_version)
    return parser.parse(mode)
'''


# STARTING RULES
# ==============

start: file

file[ast.Module]: a=[statements] ENDMARKER { ast.Module(body=a or [], type_ignores=[]) }
interactive[ast.Interactive]: a=statement_newline { ast.Interactive(body=a) }
eval[ast.Expression]: a=expressions NEWLINE* ENDMARKER { ast.Expression(body=a) }
func_type[ast.FunctionType]: '(' a=[type_expressions] ')' b=typing_arrow_return_type NEWLINE* ENDMARKER { ast.FunctionType(argtypes=a, returns=b) }
fstring[ast.Expr]: star_expressions

# DELIMITER
# =========

DELIMITER: ';' | NEWLINE

# GENERAL STATEMENTS
# ==================

statements[List[ast.stmt]]: NEWLINE* a=statement* { sum(a, []) }

statement[List[ast.stmt]]: a=expansive_stmt NEWLINE* { a } | a=compound_stmt NEWLINE* [DELIMITER] { [a] } | a=simple_stmts NEWLINE* { a }

statement_newline[List[ast.stmt]]:
    | a=compound_stmt NEWLINE { [a] }
    | simple_stmts
    | NEWLINE { [ast.Pass(LOCATIONS)] }
    | ENDMARKER { None }

simple_stmts[List[ast.stmt]]:
    # | a=simple_stmt !';' NEWLINE { [a] } # Not needed, there for speedup
    | a=DELIMITER.simple_stmt+ [DELIMITER] { a }

# NOTE: assignment MUST precede expression, else parsing a simple assignment
# will throw a SyntaxError.
simple_stmt[ast.stmt](memo):
    | assignment
    | &"type" type_alias
    | e=star_expressions { ast.Expr(value=e, LOCATIONS) }
    | &'return' return_stmt
    | &('import' | 'from') import_stmt
    | &'raise' raise_stmt
    | 'pass' { ast.Pass(LOCATIONS) }
    | &'del' del_stmt
    | &'yield' yield_stmt
    | &'assert' assert_stmt
    | 'break' { ast.Break(LOCATIONS) }
    | 'continue' { ast.Continue(LOCATIONS) }
    | &'global' global_stmt
    | &'nonlocal' nonlocal_stmt

compound_stmt:
    | &('let' | 'var') declaration_else
    | &('def' | '@' | 'async' | 'static') function_def
    | &'if' if_stmt
    | &('class' | '@') class_def
    | &('with' | 'async') with_stmt
    | &('for' | 'async') for_stmt
    | &'try' try_stmt
    | &'while' while_stmt
    | match_stmt


decl_keyword: 'var' | 'let'

# Statement expanded into several Python statements. Including single and compound.
expansive_stmt[List[ast.stmt]]:
    | &decl_keyword a=declaration_assignment !'else' [DELIMITER] { a }
    | &('with' | 'async') inline_with_stmt

# SIMPLE STATEMENTS
# =================

# NOTE: annotated_rhs may start with 'yield'; yield_expr must start with 'yield'
simple_assignment:
    | a=NAME ':' b=typing_expression c=['=' d=annotated_rhs { d }] {
        self.check_version(
            (3, 6),
            "Variable annotation syntax is",
            ast.AnnAssign(
                target=ast.Name(
                    id=a.string,
                    ctx=Store,
                    lineno=a.start[0],
                    col_offset=a.start[1],
                    end_lineno=a.end[0],
                    end_col_offset=a.end[1],
                ),
                annotation=b,
                value=c,
                simple=1,
                LOCATIONS,
            )
        ) }
    | a=('(' b=single_target ')' { b }
         | single_subscript_attribute_target) ':' b=typing_expression c=['=' d=annotated_rhs { d }] {
        self.check_version(
            (3, 6),
            "Variable annotation syntax is",
            ast.AnnAssign(
                target=a,
                annotation=b,
                value=c,
                simple=0,
                LOCATIONS,
            )
        )
     }
    | a=(z=star_targets '=' { z })+ b=(yield_expr | star_expressions) !'=' tc=[TYPE_COMMENT] {
         ast.Assign(targets=a, value=b, type_comment=tc, LOCATIONS)
     }

# NOTE: annotated_rhs may start with 'yield'; yield_expr must start with 'yield'
assignment[ast.stmt]:
    | simple_assignment
    | a=single_target b=augassign ~ c=(yield_expr | star_expressions) {
        ast.AugAssign(target = a, op=b, value=c, LOCATIONS)
     }
    | invalid_assignment


annotated_rhs: yield_expr | star_expressions

augassign:
    | '+=' { ast.Add() }
    | '-=' { ast.Sub() }
    | '*=' { ast.Mult() }
    | '@=' { self.check_version((3, 5), "The '@' operator is", ast.MatMult()) }
    | '/=' { ast.Div() }
    | '%=' { ast.Mod() }
    | '&=' { ast.BitAnd() }
    | '|=' { ast.BitOr() }
    | '^=' { ast.BitXor() }
    | '<<=' { ast.LShift() }
    | '>>=' { ast.RShift() }
    | '**=' { ast.Pow() }
    | '//=' { ast.FloorDiv() }


# Typhon let/var declaration statements
# -------------------------------------
declaration_assignment[List[ast.stmt]]:
    | d=decl_keyword a=','.declaration_single_assignment+ {
        list(map(lambda a1: assign_as_declaration(d.string, a1, len(a) > 1, LOCATIONS), a))
     }

# let-else / var-else
declaration_else:
    | d=decl_keyword pat_sub=','.pattern_assignment+ e=['else' b=block {b}] {
        make_if_let(decl_type=d.string, pattern_subjects=pat_sub, cond=None, body=[], orelse=e, is_let_else=True, LOCATIONS)
     }

declaration_single_assignment[Tuple[ast.expr, Optional[ast.expr], Optional[ast.expr]]]:
    | a=decl_star_target b=annotation c=['=' d=annotated_rhs { d }] { (a, b, c) }
    | a=decl_star_target '=' c=annotated_rhs { (a, None, c) }

pattern_assignment[Tuple[ast.pattern, ast.expr]]:
    | pattern=pattern '=' subject=star_named_expression { (pattern, subject) }

# Operator like form statements
# -----------------------------
return_stmt[ast.Return]:
    | 'return' a=[star_expressions] { ast.Return(value=a, LOCATIONS) }

raise_stmt[ast.Raise]:
    | 'raise' a=expression b=['from' z=expression { z }] { ast.Raise(exc=a, cause=b, LOCATIONS) }
    | 'raise' { ast.Raise(exc=None, cause=None, LOCATIONS) }

global_stmt[ast.Global]: 'global' a=','.NAME+ {
    ast.Global(names=[n.string for n in a], LOCATIONS)
}

nonlocal_stmt[ast.Nonlocal]: 'nonlocal' a=','.NAME+ {
    ast.Nonlocal(names=[n.string for n in a], LOCATIONS)
}

del_stmt[ast.Delete]:
    | 'del' a=del_targets &(';' | NEWLINE) { ast.Delete(targets=a, LOCATIONS) }
    | invalid_del_stmt

yield_stmt[ast.Expr]: y=yield_expr { ast.Expr(value=y, LOCATIONS) }

assert_stmt[ast.Assert]: 'assert' a=expression b=[',' z=expression { z }] {
    ast.Assert(test=a, msg=b, LOCATIONS)
}

import_stmt[ast.Import]:
    | invalid_import
    | import_name
    | import_from

# Import statements
# -----------------

import_name[ast.Import]: 'import' a=dotted_as_names { ast.Import(names=a, LOCATIONS) }

# note below: the ('.' | '...') is necessary because '...' is tokenized as ELLIPSIS
import_from[ast.ImportFrom]:
    | 'from' a=('.' | '...')* b=dotted_name 'import' c=import_from_targets {
        ast.ImportFrom(module=b, names=c, level=self.extract_import_level(a), LOCATIONS)
     }
    | 'from' a=('.' | '...')+ 'import' b=import_from_targets {
        ast.ImportFrom(names=b, level=self.extract_import_level(a), LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.ImportFrom(module=None, names=b, level=self.extract_import_level(a), LOCATIONS)
     }
import_from_targets[List[ast.alias]]:
    | '(' a=import_from_as_names [','] ')' { a }
    | import_from_as_names !','
    | '*' { [ast.alias(name="*", asname=None, LOCATIONS)] }
    | invalid_import_from_targets
import_from_as_names[List[ast.alias]]:
    | a=','.import_from_as_name+ { a }
import_from_as_name[ast.alias]:
    | a=NAME b=['as' z=NAME { z.string }] { ast.alias(name=a.string, asname=b, LOCATIONS) }
dotted_as_names[List[ast.alias]]:
    | a=','.dotted_as_name+ { a }
dotted_as_name[ast.alias]:
    | a=dotted_name b=['as' z=NAME { z.string }] { ast.alias(name=a, asname=b, LOCATIONS) }
dotted_name[str]:
    | a=dotted_name '.' b=NAME { a + "." + b.string }
    | a=NAME { a.string }

# COMPOUND STATEMENTS
# ===================

# Common elements
# ---------------

block[List[ast.stmt]] (memo):
    | '{' a=statements* '}' { sum(a, []) if len(a) > 0 else [ast.Pass(LOCATIONS)]  }
    | invalid_block

decorators: decorator+
decorator:
    | a=('@' f=dec_maybe_call NEWLINE { f }) { a }
    | a=('@' f=named_expression NEWLINE { f }) {
        self.check_version((3, 9), "Generic decorator are",  a)
     }
dec_maybe_call:
    | dn=dec_primary o=('('|'?(') z=[arguments] ')' {
        maybe_optional(
            ast.Call(func=dn, args=z[0] if z else [], keywords=z[1] if z else [], LOCATIONS),
            o.string)
     }
    | dec_primary
dec_primary:
    | a=dec_primary ('.'|'?.') b=NAME {
        maybe_optional(
            ast.Attribute(value=a, attr=b.string, ctx=Load, LOCATIONS),
            o.string)
     }
    | a=NAME { ast.Name(id=a.string, ctx=Load, LOCATIONS) }

# Class definitions
# -----------------

class_def[ast.ClassDef]:
    | a=decorators b=class_def_raw { self.set_decorators(b, a) }
    | class_def_raw

class_def_raw[ast.ClassDef]:
    | invalid_class_def_raw
    | 'class' a=NAME t=[type_params] b=['(' z=[arguments] ')' { z }] NEWLINE* c=block {
        (
            ast.ClassDef(
                a.string,
                bases=b[0] if b else [],
                keywords=b[1] if b else [],
                body=c,
                decorator_list=[],
                type_params=t or [],
                LOCATIONS,
            )
            if sys.version_info >= (3, 12) else
            ast.ClassDef(
                a.string,
                bases=b[0] if b else [],
                keywords=b[1] if b else [],
                body=c,
                decorator_list=[],
                LOCATIONS,
            )
        )
     }

# Function definitions
# --------------------

function_def[Union[ast.FunctionDef, ast.AsyncFunctionDef]]:
    | d=decorators f=function_def_raw { self.set_decorators(f, d) }
    | f=function_def_raw {self.set_decorators(f, [])}

function_def_raw[Union[ast.FunctionDef, ast.AsyncFunctionDef]]:
    | invalid_def_raw
    | st=['static'] asc=['async'] 'def' n=NAME t=[type_params] &&'(' params=[params] ')' a=[typing_arrow_return_type] tc=[func_type_comment] NEWLINE* b=block {
            make_function_def(
                is_async=(asc is not None),
                is_static=(st is not None),
                name=n.string,
                args=params or self.make_arguments(None, [], None, [], None),
                returns=a,
                body=b,
                type_comment=tc,
                type_params=t or [],
                LOCATIONS,
            )
     }


# Function parameters
# -------------------

params:
    | invalid_parameters
    | parameters

parameters[ast.arguments]:
    | a=slash_no_default b=param_no_default* c=param_with_default* d=[star_etc] {
        self.check_version(
            (3, 8), "Positional only arguments are", self.make_arguments(a, [], b, c, d)
        )
     }
    | a=slash_with_default b=param_with_default* c=[star_etc] {
        self.check_version(
            (3, 8),
            "Positional only arguments are",
            self.make_arguments(None, a, None, b, c),
        )
     }
    | a=param_no_default+ b=param_with_default* c=[star_etc] {
        self.make_arguments(None, [], a, b, c)
     }
    | a=param_with_default+ b=[star_etc] {
        self.make_arguments(None, [], None, a, b)
     }
    | a=star_etc { self.make_arguments(None, [], None, None, a) }

# Some duplication here because we can't write (',' | &')'),
# which is because we don't support empty alternatives (yet).
#

slash_no_default[List[Tuple[ast.arg, None]]]:
    | a=param_no_default+ '/' ',' { [(p, None) for p in a] }
    | a=param_no_default+ '/' &')' { [(p, None) for p in a] }
slash_with_default[List[Tuple[ast.arg, Any]]]:
    | a=param_no_default* b=param_with_default+ '/' ',' { ([(p, None) for p in a] if a else []) + b }
    | a=param_no_default* b=param_with_default+ '/' &')' { ([(p, None) for p in a] if a else []) + b }

star_etc[Tuple[Optional[ast.arg], List[Tuple[ast.arg, Any]], Optional[ast.arg]]]:
    | invalid_star_etc
    | '*' a=param_no_default b=param_maybe_default* c=[kwds] { (a, b, c) }
    | '*' a=param_no_default_star_annotation b=param_maybe_default* c=[kwds] { (a, b, c) }
    | '*' ',' b=param_maybe_default+ c=[kwds] { (None, b, c) }
    | a=kwds { (None, [], a) }

kwds[ast.arg]:
    | invalid_kwds
    | '**' a=param_no_default { a }

# One parameter.  This *includes* a following comma and type comment.
#
# There are three styles:
# - No default
# - With default
# - Maybe with default
#
# There are two alternative forms of each, to deal with type comments:
# - Ends in a comma followed by an optional type comment
# - No comma, optional type comment, must be followed by close paren
# The latter form is for a final parameter without trailing comma.
#

param_no_default[ast.arg]:
    | a=param ',' tc=TYPE_COMMENT? { self.set_arg_type_comment(a, tc) }
    | a=param tc=TYPE_COMMENT? &')' { self.set_arg_type_comment(a, tc) }
param_no_default_star_annotation[ast.arg]:
    | a=param_star_annotation ',' tc=TYPE_COMMENT? { self.set_arg_type_comment(a, tc) }
    | a=param_star_annotation tc=TYPE_COMMENT? &')' { self.set_arg_type_comment(a, tc) }
param_with_default[Tuple[ast.arg, Any]]:
    | a=param c=default ',' tc=TYPE_COMMENT? { (self.set_arg_type_comment(a, tc), c) }
    | a=param c=default tc=TYPE_COMMENT? &')' { (self.set_arg_type_comment(a, tc), c) }
param_maybe_default[Tuple[ast.arg, Any]]:
    | a=param c=default? ',' tc=TYPE_COMMENT? { (self.set_arg_type_comment(a, tc), c) }
    | a=param c=default? tc=TYPE_COMMENT? &')' { (self.set_arg_type_comment(a, tc), c) }
param: a=NAME b=annotation? { ast.arg(arg=a.string, annotation=b, LOCATIONS) }
param_star_annotation: a=NAME b=star_annotation {
    ast.arg(arg=a.string, annotations=b, LOCATIONS)
 }
annotation: ':' a=typing_expression { a }
star_annotation: ':' a=typing_star_expression { a }
default: '=' a=expression { a } | invalid_default

# If statement
# ------------

if_stmt[ast.stmt]:
    | invalid_if_stmt
    | 'if' '(' a=named_expression ')' NEWLINE* b=block c=elif_stmt { ast.If(test=a, body=b, orelse=c or [], LOCATIONS) }
    | 'if' '(' a=named_expression ')' NEWLINE* b=block c=[else_block] { ast.If(test=a, body=b, orelse=c or [], LOCATIONS) }
    # if-let statement as pattern matching
    | 'if' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=block c=elif_stmt {
        make_if_let(decl_type="let", pattern_subjects=pat_sub, cond=cond, body=b, orelse=c, is_let_else=False, LOCATIONS)
     }
    | 'if' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=block c=[else_block] {
        make_if_let(decl_type="let", pattern_subjects=pat_sub, cond=cond, body=b, orelse=c, is_let_else=False, LOCATIONS)
     }

elif_stmt[List[ast.stmt]]:
    | invalid_elif_stmt
    | 'elif' '(' a=named_expression ')' NEWLINE* b=block c=elif_stmt { [ast.If(test=a, body=b, orelse=c or [], LOCATIONS)] }
    | 'elif' '(' a=named_expression ')' NEWLINE* b=block c=[else_block] { [ast.If(test=a, body=b, orelse=c or [], LOCATIONS)] }
    # elif-let statement as pattern matching
    | 'elif' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=block c=elif_stmt {
        [make_if_let(decl_type="let", pattern_subjects=pat_sub, cond=cond, body=b, orelse=c or [], is_let_else=False, LOCATIONS)]
     }
    | 'elif' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=block c=[else_block] {
        [make_if_let(decl_type="let", pattern_subjects=pat_sub, cond=cond, body=b, orelse=c or [], is_let_else=False, LOCATIONS)]
     }
else_block[List[ast.stmt]]:
    | invalid_else_stmt
    | 'else' b=block { b }

# While statement
# ---------------

while_stmt[ast.While]:
    | invalid_while_stmt
    | 'while' '(' a=named_expression ')' NEWLINE* b=block c=[else_block] {
        ast.While(test=a, body=b, orelse=c or [], LOCATIONS)
     }
    # while-let statement as pattern matching
    | 'while' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=block c=[else_block] {
        make_while_let(pattern_subjects=pat_sub, cond=cond, body=b, orelse=c, LOCATIONS)
     }

# For statement
# -------------

for_stmt[Union[ast.For, ast.AsyncFor]]:
    | invalid_for_stmt
    | is_async=['async'] 'for' '(' d=decl_keyword t=decl_star_target a=[star_annotation] 'in' ~ ex=star_expressions ')' tc=[TYPE_COMMENT] NEWLINE* b=block el=[else_block] {
        make_for_stmt(decl_type=d.string, target=t, type_annotation=a, iter=ex, body=b, orelse=el or [], type_comment=tc, is_async=is_async is not None, LOCATIONS)
     }
    | is_async=['async'] 'for' '(' d=decl_keyword pat=pattern 'in' ~ ex=star_expressions ')' tc=[TYPE_COMMENT] NEWLINE* b=block el=[else_block] {
        make_for_let_pattern(decl_type=d.string, pattern=pat,iter=ex, body=b, orelse=el or [], type_comment=tc, is_async=is_async is not None, LOCATIONS)
     }
    | invalid_for_target

# With statement
# --------------

with_stmt[Union[ast.With, ast.AsyncWith]]:
    | block_with
    | invalid_with_stmt

block_with[Union[ast.With, ast.AsyncWith]]:
    | is_async=['async'] 'with' '(' a=with_item ')' NEWLINE* b=block {
        make_with_stmt(is_async=is_async is not None, items=a, body=b, is_inline=False, LOCATIONS)
     }
    | is_async=['async'] 'with' '(' a=with_pattern_item ')' NEWLINE* b=block {
        make_with_let_pattern(is_async=is_async is not None, decl_type=a[0], pattern_subjects=a[1], body=b, LOCATIONS)
     }

inline_with_stmt[List[ast.stmt]]:
    | is_async=['async'] 'with' a=with_item DELIMITER {
        [make_with_stmt(is_async=is_async is not None, items=a, body=[], is_inline=True, LOCATIONS)]
     }
    | is_async=['async'] 'with' a=with_pattern_item DELIMITER {
        make_inline_with_let_pattern(is_async=is_async is not None, decl_type=a[0], pattern_subjects=a[1], LOCATIONS)
     }

with_item[List[ast.withitem]]:
    | a=declaration_assignment &(',' | ')' | DELIMITER) { list(map(declaration_as_withitem, a)) }
    | invalid_with_item
    | exprs=','.expression+ [','] {
        list(map(lambda e: ast.withitem(context_expr=e, optional_vars=None), exprs))
     }

with_pattern_item[Tuple[str, List[Tuple[ast.pattern, ast.expr]]]]:
    | d=decl_keyword pat_sub=','.pattern_assignment+ &(',' | ')' | DELIMITER) { (d.string, pat_sub) }

# Try statement
# -------------

try_stmt[ast.Try]:
    | invalid_try_stmt
    | 'try' b=block f=finally_block {
        ast.Try(body=b, handlers=[], orelse=[], finalbody=f, LOCATIONS)
     }
    | 'try' b=block ex=except_block+ el=[else_block] f=[finally_block] {
        ast.Try(body=b, handlers=ex, orelse=el or [], finalbody=f or [], LOCATIONS)
     }
    | 'try' b=block ex=except_star_block+ el=[else_block] f=[finally_block] {
        self.check_version(
            (3, 11),
            "Exception groups are",
            (
                ast.TryStar(body=b, handlers=ex, orelse=el or [], finalbody=f or [], LOCATIONS)
                if sys.version_info >= (3, 11)
                else None
            )
        )
     }

# Except statement
# ----------------

except_block[ast.ExceptHandler]:
    | 'except' '(' e=expression t=['as' z=NAME { z.string }] ')' NEWLINE* b=block {
        ast.ExceptHandler(type=e, name=t, body=b, LOCATIONS) }
    | 'except' b=block { ast.ExceptHandler(type=None, name=None, body=b, LOCATIONS) }
    | invalid_except_stmt
except_star_block[ast.ExceptHandler]:
    | 'except' '*' '(' e=expression t=['as' z=NAME { z.string }]')' NEWLINE* b=block {
        ast.ExceptHandler(type=e, name=t, body=b, LOCATIONS)
     }
    | invalid_except_stmt
finally_block[List[ast.stmt]]:
    | invalid_finally_stmt
    | 'finally' a=block { a }

# Match statement
# ---------------

# We cannot do version checks here since the production will occur after any other
# production which will have failed since the ast module does not have the right nodes.
match_stmt["ast.Match"]:
    | "match" '(' subject=subject_expr ')' NEWLINE* '{' cases=case_block+  '}' {
        ast.Match(subject=subject, cases=cases, LOCATIONS)
     }
    | invalid_match_stmt

# Version checking here allows to avoid tracking down every single possible production
subject_expr:
    | value=star_named_expression ',' values=star_named_expressions? {
        self.check_version(
            (3, 10),
            "Pattern matching is",
            ast.Tuple(elts=[value] + (values or []), ctx=Load, LOCATIONS)
        )
     }
    | e=named_expression { self.check_version((3, 10), "Pattern matching is", e)}

case_block["ast.match_case"]:
    | invalid_case_block
    | "case" '(' pattern=patterns ')' NEWLINE* guard=guard? NEWLINE* body=block {
        ast.match_case(pattern=pattern, guard=guard, body=body)
     }

guard: 'if' '(' guard=named_expression ')' { guard }

patterns:
    | patterns=open_sequence_pattern {
        make_tuple_pattern(patterns=patterns, LOCATIONS)
     }
    | pattern

pattern:
    | as_pattern
    | or_pattern

as_pattern["ast.MatchAs"]:
    | pattern=or_pattern 'as' target=pattern_capture_target_annot {
        set_type_annotation(ast.MatchAs(pattern=pattern, name=target[0], LOCATIONS), target[1])
     }
    | invalid_as_pattern

or_pattern["ast.MatchOr"]:
    | patterns='|'.closed_pattern+ {
        ast.MatchOr(patterns=patterns, LOCATIONS) if len(patterns) > 1 else patterns[0]
     }

closed_pattern:
    | literal_pattern
    | capture_pattern
    | wildcard_pattern
    | value_pattern
    | group_pattern
    | sequence_pattern
    | mapping_pattern
    | class_pattern
    | attributes_pattern

# Literal patterns are used for equality and identity constraints
literal_pattern:
    | value=signed_number !('+' | '-') { ast.MatchValue(value=value, LOCATIONS) }
    | value=complex_number { ast.MatchValue(value=value, LOCATIONS) }
    | value=strings { ast.MatchValue(value=value, LOCATIONS) }
    | 'None' { ast.MatchSingleton(value=None, LOCATIONS) }
    | 'True' { ast.MatchSingleton(value=True, LOCATIONS) }
    | 'False' { ast.MatchSingleton(value=False, LOCATIONS) }

# Literal expressions are used to restrict permitted mapping pattern keys
literal_expr:
    | signed_number !('+' | '-')
    | complex_number
    | strings
    | 'None' { ast.Constant(value=None, LOCATIONS) }
    | 'True' { ast.Constant(value=True, LOCATIONS) }
    | 'False' { ast.Constant(value=False, LOCATIONS) }

complex_number:
    | real=signed_real_number '+' imag=imaginary_number {
        ast.BinOp(left=real, op=ast.Add(), right=imag, LOCATIONS)
     }
    | real=signed_real_number '-' imag=imaginary_number  {
        ast.BinOp(left=real, op=ast.Sub(), right=imag, LOCATIONS)
     }

signed_number:
    | a=NUMBER { ast.Constant(value=ast.literal_eval(a.string), LOCATIONS) }
    | '-' a=NUMBER {
        ast.UnaryOp(
            op=ast.USub(),
            operand=ast.Constant(
                value=ast.literal_eval(a.string),
                lineno=a.start[0],
                col_offset=a.start[1],
                end_lineno=a.end[0],
                end_col_offset=a.end[1]
            ),
            LOCATIONS,
        )
     }

signed_real_number:
    | real_number
    | '-' real=real_number { ast.UnaryOp(op=ast.USub(), operand=real, LOCATIONS) }

real_number[ast.Constant]:
    | real=NUMBER { ast.Constant(value=self.ensure_real(real), LOCATIONS) }

imaginary_number[ast.Constant]:
    | imag=NUMBER { ast.Constant(value=self.ensure_imaginary(imag), LOCATIONS) }

capture_pattern:
    | target=pattern_capture_target_annot {
        set_type_annotation(ast.MatchAs(pattern=None, name=target[0], LOCATIONS), target[1])
     }

# Allow '=' after name for if-net statement
# pattern_capture_target[str]:
#     | !"_" name=NAME !('.' | '(' | '=') { name.string }
pattern_capture_target[str]:
    | !"_" name=NAME !('.' | '(') { name.string }

pattern_capture_target_annot[tuple[str, ast.expr | None]]:
    | !"_" name=NAME a=annotation? !('.' | '(') { (name.string, a) }

wildcard_pattern["ast.MatchAs"]:
    | "_" { ast.MatchAs(pattern=None, name=None, LOCATIONS) }

# Allow '=' after name for if-net statement
# value_pattern["ast.MatchValue"]:
#     | attr=attr !('.' | '(' | '=') { ast.MatchValue(value=attr, LOCATIONS) }
value_pattern["ast.MatchValue"]:
    | attr=attr !('.' | '(') { ast.MatchValue(value=attr, LOCATIONS) }

attr[ast.Attribute]:
    | value=name_or_attr ('.'|'?.') attr=NAME {
        maybe_optional(
            ast.Attribute(value=value, attr=attr.string, ctx=Load, LOCATIONS),
            o.string)
    }

name_or_attr:
    | attr
    | name=NAME { ast.Name(id=name.string, ctx=Load, LOCATIONS) }

group_pattern:
    | '(' pattern=pattern ')' { pattern }

sequence_pattern["ast.MatchSequence"]:
    | '[' patterns=maybe_sequence_pattern? ']' { ast.MatchSequence(patterns=patterns or [], LOCATIONS) }
    | '(' patterns=open_sequence_pattern? ')' { make_tuple_pattern(patterns=patterns or [], LOCATIONS) }

open_sequence_pattern:
    | pattern=maybe_star_pattern ',' patterns=maybe_sequence_pattern? {
        [pattern] + (patterns or [])
     }
     # TODO: invalid case for the case ',' does not exist?

maybe_sequence_pattern:
    | patterns=','.maybe_star_pattern+ ','? { patterns }

maybe_star_pattern:
    | star_pattern
    | pattern

star_pattern:
    | '*' target=pattern_capture_target_annot {
        set_type_annotation(ast.MatchStar(name=target[0], LOCATIONS), target[1])
     }
    | '*' wildcard_pattern { ast.MatchStar(target=None, LOCATIONS) }

mapping_pattern:
    | '{' '}' { ast.MatchMapping(keys=[], patterns=[], rest=None, LOCATIONS) }
    | '{' rest=double_star_pattern ','? '}' {
        ast.MatchMapping(keys=[], patterns=[], rest=rest, LOCATIONS) }
    | '{' items=items_pattern ',' rest=double_star_pattern ','? '}' {
        ast.MatchMapping(
            keys=[k for k,_ in items],
            patterns=[p for _, p in items],
            rest=rest,
            LOCATIONS,
        )
     }
    | '{' items=items_pattern ','? '}' {
        ast.MatchMapping(
            keys=[k for k,_ in items],
            patterns=[p for _, p in items],
            rest=None,
            LOCATIONS,
        )
     }

items_pattern:
    | ','.key_value_pattern+

# In pattern '?.' is not allowed
attr_pattern[ast.Attribute]:
    | value=name_or_attr '.' attr=NAME {
        ast.Attribute(value=value, attr=attr.string, ctx=Load, LOCATIONS)
    }

name_or_attr_pattern:
    | attr_pattern
    | name=NAME { ast.Name(id=name.string, ctx=Load, LOCATIONS) }

key_value_pattern:
    | key=(literal_expr | attr_pattern) ':' pattern=pattern { (key, pattern) }

double_star_pattern:
    | '**' target=pattern_capture_target { target }

class_pattern["ast.MatchClass"]:
    | cls=name_or_attr_pattern '(' ')' {
        ast.MatchClass(cls=cls, patterns=[], kwd_attrs=[], kwd_patterns=[], LOCATIONS)
     }
    | cls=name_or_attr_pattern '(' patterns=positional_patterns ','? ')' {
        ast.MatchClass(cls=cls, patterns=patterns, kwd_attrs=[], kwd_patterns=[], LOCATIONS)
     }
    | cls=name_or_attr_pattern '(' keywords=keyword_patterns ','? ')' {
        ast.MatchClass(
            cls=cls,
            patterns=[],
            kwd_attrs=[k for k, _ in keywords],
            kwd_patterns=[p for _, p in keywords],
            LOCATIONS,
        )
     }
    | cls=name_or_attr_pattern '(' patterns=positional_patterns ',' keywords=keyword_patterns ','? ')' {
        ast.MatchClass(
            cls=cls,
            patterns=patterns,
            kwd_attrs=[k for k, _ in keywords],
            kwd_patterns=[p for _, p in keywords],
            LOCATIONS,
        )
     }
    | invalid_class_pattern

attribute_capture_pattern[tuple[str, ast.pattern | None]]:
    | '.' p=keyword_pattern { p }
    | '.' a=NAME { (a.string, None) }

attributes_pattern["ast.MatchClass"]:
    | '{' keywords=','.attribute_capture_pattern+ ','? '}' {
        make_attributes_pattern(keywords=keywords, LOCATIONS)
     }

# positional pattern is used only in class pattern.
positional_patterns:
    | args=','.positional_pattern+ { args }

# In positional patterns, Typhon does not allow '=' after NAME in
# positional_pattern_capture_target and positional_value_pattern
# in order to avoid prefix ambiguity in the boundary between
# positional and keyword patterns.
# This is same as the behavior of normal pattern in original Python's sytanx.
# But in Typhon we use pattern to be the RHS of if-let statement, where
# '=' comes after NAME necessarily.
positional_pattern:
    | positional_as_pattern
    | positional_or_pattern

positional_as_pattern["ast.MatchAs"]:
    | pattern=positional_or_pattern 'as' target=positional_pattern_capture_target_annot {
        set_type_annotation(ast.MatchAs(pattern=pattern, name=target[0], LOCATIONS), target[1])
     }
    | invalid_as_pattern

positional_or_pattern["ast.MatchOr"]:
    | patterns='|'.positional_closed_pattern+ {
        ast.MatchOr(patterns=patterns, LOCATIONS) if len(patterns) > 1 else patterns[0]
     }

positional_pattern_capture_target[str]:
    | !"_" name=NAME !('.' | '(' | '=') { name.string }

positional_pattern_capture_target_annot[tuple[str, ast.expr | None]]:
    | !"_" name=NAME a=annotation? !('.' | '(' | '=') { (name.string, a) }

positional_capture_pattern:
    | target=positional_pattern_capture_target_annot {
        set_type_annotation(ast.MatchAs(pattern=None, name=target[0], LOCATIONS), target[1])
     }

positional_value_pattern["ast.MatchValue"]:
    | attr=attr !('.' | '(' | '=') { ast.MatchValue(value=attr, LOCATIONS) }

positional_closed_pattern:
    | literal_pattern
    | positional_capture_pattern
    | wildcard_pattern
    | positional_value_pattern
    | group_pattern
    | sequence_pattern
    | mapping_pattern
    | class_pattern

keyword_patterns:
    | ','.keyword_pattern+

keyword_pattern:
    | arg=NAME '=' value=pattern { (arg.string, value) }

# Type statement
# ---------------

type_alias["ast.TypeAlias"]:
    | "type" n=NAME t=[type_params] '=' b=typing_expression {
        self.check_version(
            (3, 12),
            "Type statement is",
            (
                ast.TypeAlias(
                    name=ast.Name(
                        id=n.string,
                        ctx=Store,
                        lineno=n.start[0],
                        col_offset=n.start[1],
                        end_lineno=n.end[0],
                        end_col_offset=n.end[1],
                    ),
                    type_params=t or [],
                    value=b,
                    LOCATIONS
                )
                if sys.version_info >= (3, 12)
                else None
            )
        )
     }

# Type parameter declaration
# --------------------------

type_params[List[ast.type_param]]: '[' t=type_param_seq  ']' {
    self.check_version(
        (3, 12),
        "Type parameter lists are",
        t
    )
 }

type_param_seq: a=','.type_param+ [','] { a }

type_param[ast.type_param](memo):
    | a=NAME b=[type_param_bound] {
        ast.TypeVar(name=a.string, bound=b, LOCATIONS)
        if sys.version_info >= (3, 12)
        else object()
     }
    | '*' a=NAME colon=":" e=typing_expression {
        self.raise_syntax_error_starting_from(
            "cannot use constraints with TypeVarTuple"
            if isinstance(e, ast.Tuple)
            else "cannot use bound with TypeVarTuple",
            colon
        )
     }
    | '*' a=NAME {
        ast.TypeVarTuple(name=a.string, LOCATIONS)
        if sys.version_info >= (3, 12)
        else object()
     }
    | '**' a=NAME colon=":" e=typing_expression {
        self.raise_syntax_error_starting_from(
            "cannot use constraints with ParamSpec"
            if isinstance(e, ast.Tuple)
            else "cannot use bound with ParamSpec",
            colon
        )
     }
    | '**' a=NAME {
        ast.ParamSpec(name=a.string, LOCATIONS)
        if sys.version_info >= (3, 12)
        else object()
     }

type_param_bound: ":" e=typing_expression { e }

# TYPING EXPRESSION (EXPRESSION without 'in', 'is', 'lambda').
typing_star_expression (memo):
    | '*' a=bitwise_or { ast.Starred(value=a, ctx=Load, LOCATIONS) }
    | typing_expression

typing_expression[ast.expr](memo):
    | t=typing_expression_impl { set_is_typing_expression(t) }

typing_expression_impl[ast.expr](memo):
    | invalid_expression
    | &'(' typing_arrow_type
    #| typing_disjunction
    | typing_arrow_single_param_type

typing_arrow_single_param_type[ast.expr]:
    | !'(' a=typing_disjunction r=('->' a=typing_arrow_single_param_type { a })+ {
        make_arrow_type_single_chain(a, None, r, LOCATIONS)
     }
    | typing_disjunction

typing_disjunction[ast.expr](memo):
    | a=typing_conjunction b=('||' c=typing_conjunction { c })+ { ast.BoolOp(op=ast.Or(), values=[a] + b, LOCATIONS) }
    | typing_conjunction

typing_conjunction[ast.expr](memo):
    | a=typing_inversion b=('&&' c=typing_inversion { c })+ { ast.BoolOp(op=ast.And(), values=[a] + b, LOCATIONS) }
    | typing_inversion

typing_inversion[ast.expr](memo):
    | '!' a=typing_inversion { ast.UnaryOp(op=ast.Not(), operand=a, LOCATIONS) }
    | typing_comparison

typing_comparison[ast.expr]:
    | a=bitwise_or b=typing_compare_op_bitwise_or_pair+ {
        ast.Compare(left=a, ops=self.get_comparison_ops(b), comparators=self.get_comparators(b), LOCATIONS)
     }
    | bitwise_or

typing_compare_op_bitwise_or_pair[Tuple[ast.cmpop, ast.expr]]:
    | eq_bitwise_or
    | noteq_bitwise_or
    | lte_bitwise_or
    | lt_bitwise_or
    | gte_bitwise_or
    | gt_bitwise_or
    # No is_bitwise_or nor in_bitwise_or

# Does not include default arguments.
typing_arrow_param_annot_or_type[ast.arg]:
    | a=NAME b=annotation { ast.arg(arg=a.string, annotation=b, LOCATIONS) }
    | b=typing_expression { ast.arg(arg="", annotation=b, LOCATIONS) }

typing_arrow_param[ast.arg]:
    | a=typing_arrow_param_annot_or_type ',' { a }
    | a=typing_arrow_param_annot_or_type &')' { a }

typing_arrow_kwds[ast.arg]:
    | '**' a=typing_arrow_param { a }

typing_arrow_param_star_annotation:
    | a=param_star_annotation ',' tc=TYPE_COMMENT? { self.set_arg_type_comment(a, tc) }
    | a=param_star_annotation tc=TYPE_COMMENT? &')' { self.set_arg_type_comment(a, tc) }

typing_arrow_star_etc[Tuple[ast.arg, ast.arg]]:
    | '*' a=typing_arrow_param c=[typing_arrow_kwds] { (a,c) }
    | '*' a=typing_arrow_param_star_annotation c=[typing_arrow_kwds] { (a,c) }

typing_arrow_return_type[ast.expr]:
    | '->' a=typing_expression { a }

typing_arrow_type[ast.expr]:
    | '(' a=typing_arrow_param* c=[typing_arrow_star_etc] ')' r=typing_arrow_return_type {
        make_arrow_type(a, c, r, LOCATIONS)
    }

# EXPRESSIONS
# -----------

expressions:
    | '(' a=expression b=(',' c=expression { c })+ [','] ')' {
        ast.Tuple(elts=[a] + b, ctx=Load, LOCATIONS) }
    | a=expression ',' { ast.Tuple(elts=[a], ctx=Load, LOCATIONS) }
    | expression

expression (memo):
    | invalid_expression
    | pipe_operator

yield_expr:
    | 'yield' 'from' a=expression { ast.YieldFrom(value=a, LOCATIONS) }
    | 'yield' a=[star_expressions] { ast.Yield(value=a, LOCATIONS) }

star_expressions:
    | '(' a=star_expression b=(',' c=star_expression { c })+ [','] ')' {
        ast.Tuple(elts=[a] + b, ctx=Load, LOCATIONS) }
    | '(' a=star_expression ',' ')' { ast.Tuple(elts=[a], ctx=Load, LOCATIONS) }
    | star_expression

star_expression (memo):
    | '*' a=bitwise_or { ast.Starred(value=a, ctx=Load, LOCATIONS) }
    | expression

star_named_expressions: a=','.star_named_expression+ [','] { a }

star_named_expression:
    | '*' a=bitwise_or { ast.Starred(value=a, ctx=Load, LOCATIONS) }
    | named_expression

assignment_expression:
    | a=NAME ':=' ~ b=expression {
        self.check_version(
            (3, 8),
            "The ':=' operator is",
            ast.NamedExpr(
                target=ast.Name(
                    id=a.string,
                    ctx=Store,
                    lineno=a.start[0],
                    col_offset=a.start[1],
                    end_lineno=a.end[0],
                    end_col_offset=a.end[1]
                ),
                value=b,
                LOCATIONS,
            )
        )
     }

named_expression:
    | assignment_expression
    | invalid_named_expression
    | a=expression !':=' { a }

pipe_operator (memo):
    | a=coalescing op_b=(op=('|>'|'?|>') c=coalescing { (op.string == '?|>', c) })+ {
        make_pipe_call(a, op_b, LOCATIONS)
     }
    | coalescing

coalescing (memo):
    | a=arrow_single_param b=('??' c=arrow_single_param { c })+ {
        set_is_coalescing(ast.Tuple(elts=[a] + b, ctx=Load, LOCATIONS))
     }
    | arrow_single_param
# coalescing (memo):
#     | a=disjunction b=('??' c=disjunction { c })+ {
#         set_is_coalescing(ast.Tuple(elts=[a] + b, ctx=Load, LOCATIONS))
#      }
#     | disjunction

arrow_single_param[ast.expr]:
    | !'(' a=disjunction r=('->' a=typing_expression { a })+ {
        make_arrow_type_single_chain(a, None, r, LOCATIONS)
     }
    | disjunction

disjunction (memo):
    | a=conjunction b=('||' c=conjunction { c })+ { ast.BoolOp(op=ast.Or(), values=[a] + b, LOCATIONS) }
    | conjunction

conjunction (memo):
    | a=comparison b=('&&' c=comparison { c })+ { ast.BoolOp(op=ast.And(), values=[a] + b, LOCATIONS) }
    | comparison

# Comparisons operators
# ---------------------

comparison:
    | a=bitwise_or b=compare_op_bitwise_or_pair+ {
        ast.Compare(left=a, ops=self.get_comparison_ops(b), comparators=self.get_comparators(b), LOCATIONS)
     }
    | bitwise_or

# Make a tuple of operator and comparator
compare_op_bitwise_or_pair[Tuple[ast.cmpop, ast.expr]]:
    | eq_bitwise_or
    | noteq_bitwise_or
    | lte_bitwise_or
    | lt_bitwise_or
    | gte_bitwise_or
    | gt_bitwise_or
    | notin_bitwise_or
    | in_bitwise_or
    | isnot_bitwise_or
    | is_bitwise_or

eq_bitwise_or[Tuple[ast.cmpop, ast.expr]]: '==' a=bitwise_or { (ast.Eq(), a) }
# Do not support the Barry as BDFL <> for not eq
noteq_bitwise_or[Tuple[ast.cmpop, ast.expr]]:
    | '!=' a=bitwise_or { (ast.NotEq(), a) }
lte_bitwise_or[Tuple[ast.cmpop, ast.expr]]: '<=' a=bitwise_or { (ast.LtE(), a) }
lt_bitwise_or[Tuple[ast.cmpop, ast.expr]]: '<' a=bitwise_or { (ast.Lt(), a) }
gte_bitwise_or[Tuple[ast.cmpop, ast.expr]]: '>=' a=bitwise_or { (ast.GtE(), a) }
gt_bitwise_or[Tuple[ast.cmpop, ast.expr]]: '>' a=bitwise_or { (ast.Gt(), a) }
notin_bitwise_or[Tuple[ast.cmpop, ast.expr]]: 'not' 'in' a=bitwise_or { (ast.NotIn(), a) }
in_bitwise_or[Tuple[ast.cmpop, ast.expr]]: 'in' a=bitwise_or { (ast.In(), a) }
isnot_bitwise_or[Tuple[ast.cmpop, ast.expr]]: 'is' 'not' a=bitwise_or { (ast.IsNot(), a) }
is_bitwise_or[Tuple[ast.cmpop, ast.expr]]: 'is' a=bitwise_or { (ast.Is(), a) }

# Logical operators
# -----------------

bitwise_or:
    | a=bitwise_or '|' b=bitwise_xor { ast.BinOp(left=a, op=ast.BitOr(), right=b, LOCATIONS) }
    | bitwise_xor

bitwise_xor:
    | a=bitwise_xor '^' b=bitwise_and { ast.BinOp(left=a, op=ast.BitXor(), right=b, LOCATIONS) }
    | bitwise_and

bitwise_and:
    | a=bitwise_and '&' b=shift_expr { ast.BinOp(left=a, op=ast.BitAnd(), right=b, LOCATIONS) }
    | shift_expr

shift_expr:
    | a=shift_expr '<<' b=sum { ast.BinOp(left=a, op=ast.LShift(), right=b, LOCATIONS) }
    | a=shift_expr '>>' b=sum { ast.BinOp(left=a, op=ast.RShift(), right=b, LOCATIONS) }
    | sum

# Arithmetic operators
# --------------------

sum:
    | a=sum '+' b=term { ast.BinOp(left=a, op=ast.Add(), right=b, LOCATIONS) }
    | a=sum '-' b=term { ast.BinOp(left=a, op=ast.Sub(), right=b, LOCATIONS) }
    | term

term:
    | a=term '*' b=factor { ast.BinOp(left=a, op=ast.Mult(), right=b, LOCATIONS) }
    | a=term '/' b=factor { ast.BinOp(left=a, op=ast.Div(), right=b, LOCATIONS) }
    | a=term '//' b=factor { ast.BinOp(left=a, op=ast.FloorDiv(), right=b, LOCATIONS) }
    | a=term '%' b=factor { ast.BinOp(left=a, op=ast.Mod(), right=b, LOCATIONS) }
    | a=term '@' b=factor {
        self.check_version((3, 5), "The '@' operator is", ast.BinOp(left=a, op=ast.MatMult(), right=b, LOCATIONS))
     }
    | factor

factor (memo):
    | '+' a=factor { ast.UnaryOp(op=ast.UAdd(), operand=a, LOCATIONS) }
    | '-' a=factor { ast.UnaryOp(op=ast.USub(), operand=a, LOCATIONS) }
    | '~' a=factor { ast.UnaryOp(op=ast.Invert(), operand=a, LOCATIONS) }
    | power

power:
    | a=optional_question '**' b=factor { ast.BinOp(left=a, op=ast.Pow(), right=b, LOCATIONS) }
    | optional_question

# Unary operators
# ---------------

optional_question (memo): # postfix `?`
    | a=force_unwrap '_typh_op_optional_question' { get_optional_question_node(a, LOCATIONS) }
    | force_unwrap

force_unwrap (memo): # postfix `!`
    | a=inversion '_typh_op_force_unwrap' { get_force_unwrap_node(a, LOCATIONS) }
    | inversion

inversion (memo):
    | '!' a=inversion { ast.UnaryOp(op=ast.Not(), operand=a, LOCATIONS) }
    | await_primary

# Primary elements
# ----------------

# Primary elements are things like "obj.something.something", "obj[something]", "obj(something)", "obj" ...

await_primary (memo):
    | 'await' a=primary { self.check_version((3, 5), "Await expressions are", ast.Await(a, LOCATIONS)) }
    | primary

# ? for optional?
primary:
    | a=primary o=('.' | '?.') b=NAME {
        maybe_optional(
            ast.Attribute(value=a, attr=b.string, ctx=Load, LOCATIONS),
            o.string)
     }
    | a=primary b=comprehension_paren { ast.Call(func=a, args=[b], keywords=[], LOCATIONS) }
    | a=primary b=question_comprehension_paren {
        maybe_optional(ast.Call(func=a, args=[b], keywords=[], LOCATIONS), '?')
     }
    | a=primary o=('('|'?(') b=[arguments] ')' {
        maybe_optional(
            ast.Call(
                func=a,
                args=b[0] if b else [],
                keywords=b[1] if b else [],
                LOCATIONS,
            ),
            o.string)
     }
    | a=primary o=('['|'?[') b=slices ']' {
        maybe_optional(
            ast.Subscript(value=a, slice=b, ctx=Load, LOCATIONS),
            o.string)
     }
    | atom

slices:
    | a=slice !',' { a }
    | a=','.(slice | starred_expression)+ [','] {
        ast.Tuple(elts=a, ctx=Load, LOCATIONS)
        if sys.version_info >= (3, 9) else
        (
            ast.ExtSlice(dims=a, LOCATIONS)
            if any(isinstance(e, ast.Slice) for e in a) else
            ast.Index(value=ast.Tuple(elts=[e.value for e in a], ctx=Load, LOCATIONS), LOCATIONS)
        )
     }

slice:
    | a=[expression] ':' b=[expression] c=[':' d=[expression] { d }] {
        ast.Slice(lower=a, upper=b, step=c, LOCATIONS)
     }
    | a=named_expression {
        a
        if sys.version_info >= (3, 9) or isinstance(a, ast.Slice) else
        ast.Index(
            value=a,
            lineno=a.lineno,
            col_offset=a.col_offset,
            end_lineno=a.end_lineno,
            end_col_offset=a.end_col_offset
        )
     }

atom:
    | a=NAME { ast.Name(id=a.string, ctx=Load, LOCATIONS) }
    | 'True' {
        ast.Constant(value=True, LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.Constant(value=True, kind=None, LOCATIONS)
     }
    | 'False' {
        ast.Constant(value=False, LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.Constant(value=False, kind=None, LOCATIONS)
     }
    | 'None' {
        ast.Constant(value=None, LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.Constant(value=None, kind=None, LOCATIONS)
     }
    | &(STRING|FSTRING_START) strings
    | a=NUMBER {
        ast.Constant(value=ast.literal_eval(a.string), LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.Constant(value=ast.literal_eval(a.string), kind=None, LOCATIONS)
     }
    | &'(' (function_literal | typing_arrow_type | tuple | group | comprehension_paren)
    | &'[' (list | listcomp)
    | &'{' (dict | set | dictcomp | setcomp)
    | &'{|'(record_type | record_literal)
    | '...' {
        ast.Constant(value=Ellipsis, LOCATIONS)
        if sys.version_info >= (3, 9) else
        ast.Constant(value=Ellipsis, kind=None, LOCATIONS)
     }

group:
    | '(' a=(yield_expr | named_expression) ')' { a }
    | invalid_group


function_literal:
    | '(' p=params ')' a=[typing_arrow_return_type] '=>' ~ b=(block | expression) { make_function_literal(args=p, returns=a, body=b, LOCATIONS) }

# LITERALS
# ========

fstring_mid[ast.expr]:
    | fstring_replacement_field
    | t=FSTRING_MIDDLE { ast.Constant(value=t.string, LOCATIONS) }
fstring_replacement_field[ast.FormattedValue]:
    | '{' a=(yield_expr | star_expressions) debug_expr="="? conversion=[fstring_conversion] format=[fstring_full_format_spec] rbrace='}' {
        ast.FormattedValue(
            value=a,
            conversion=(
                conversion.string.encode()[0]
                if conversion else
                (b'r'[0] if debug_expr else -1)
            ),
            format_spec=format,
            LOCATIONS
        )
     }
    | invalid_replacement_field
fstring_conversion[int]:
    | conv_token="!" conv=NAME { self.check_fstring_conversion(conv_token, conv) }
fstring_full_format_spec:
    | ':' spec=fstring_format_spec* {
        ast.JoinedStr(
            values=spec if spec and (len(spec) > 1 or spec[0].value) else [],
            LOCATIONS,
        )
     }
fstring_format_spec:
    | t=FSTRING_MIDDLE { ast.Constant(value=t.string, LOCATIONS) }
    | fstring_replacement_field
fstring[ast.JoinedStr]:
    | a=FSTRING_START b=fstring_mid* c=FSTRING_END {
        ast.JoinedStr(values=b, LOCATIONS)
     }

strings (memo): a=(fstring|STRING)+ {
    self.concatenate_strings(a)
 }

list[ast.List]:
    | '[' a=[star_named_expressions] ']' { ast.List(elts=a or [], ctx=Load, LOCATIONS) }

tuple[ast.Tuple]:
    | '(' a=[y=star_named_expression ',' z=[star_named_expressions] { [y] + (z or []) } ] ')' {
        ast.Tuple(elts=a or [], ctx=Load, LOCATIONS)
     }

set[ast.Set]: '{' a=star_named_expressions '}' { ast.Set(elts=a, LOCATIONS) }

# Dicts
# -----

dict[ast.Dict]:
    | '{' a=[double_starred_kvpairs] '}' {
        ast.Dict(keys=[kv[0] for kv in (a or [])], values=[kv[1] for kv in (a or [])], LOCATIONS)
     }
    | '{' invalid_double_starred_kvpairs '}'

double_starred_kvpairs[List[Tuple[ast.expr | None, ast.expr]]]:
    | a=','.double_starred_kvpair+ [','] { a }

double_starred_kvpair[Tuple[ast.expr | None, ast.expr]]:
    | '**' a=bitwise_or { (None, a) }
    | kvpair

kvpair[Tuple[ast.expr, ast.expr]]: a=expression ':' b=expression { (a, b) }

# Record
# ------

record_literal_field[Tuple[ast.Name, ast.expr | None, ast.expr]]:
    | a=NAME b=annotation? '=' c=expression { (ast.Name(id=a.string, LOCATIONS), b, c) }

record_literal[ast.Name]:
    | '{|' fs=','.record_literal_field+ [','] '|}' { make_record_literal(fs, LOCATIONS) }

record_type_field[Tuple[ast.Name, ast.expr]]:
    | a=NAME b=annotation { (ast.Name(id=a.string, LOCATIONS), b) }

record_type[ast.Name]:
    | '{|' fs=','.record_type_field+ [','] '|}' { make_record_type(fs, LOCATIONS) }

# Comprehensions & Generators
# ---------------------------

for_if_clauses[List[ast.comprehension]]:
    | a=for_if_clause+ { a }

for_if_clause[ast.comprehension]:
    | 'async' 'for' '(' d=decl_keyword t=decl_star_target a=[star_annotation] 'in' ~ b=pipe_operator ')' NEWLINE* c=('if' '(' z=pipe_operator ')' NEWLINE* { z })* {
        self.check_version(
            (3, 6),
            "Async comprehensions are",
            make_comprehension(decl_type=d.string, target=t, type_annotation=a, iter=b, ifs=c, is_async=1)
        )
     }
    | 'for' '(' d=decl_keyword t=decl_star_target a=[star_annotation] 'in' ~ b=pipe_operator ')' NEWLINE* c=('if' '(' z=pipe_operator ')' NEWLINE* { z })* {
        make_comprehension(decl_type=d.string, target=t, type_annotation=a, iter=b, ifs=c, is_async=0) }
    | invalid_for_target

listcomp[ast.ListComp]:
    | '[' b=for_if_clauses 'yield' ~ a=named_expression ']' { make_listcomp(elt=a, generators=b, LOCATIONS) }
    | invalid_comprehension

setcomp[ast.SetComp]:
    | '{' b=for_if_clauses 'yield' ~ a=named_expression '}' { make_setcomp(elt=a, generators=b, LOCATIONS) }
    | invalid_comprehension

genexp_internal[ast.GeneratorExp]:
    | b=for_if_clauses 'yield' ~ a=( assignment_expression | expression !':=')  {
        make_genexp(elt=a, generators=b, LOCATIONS)
      }

genexp[ast.GeneratorExp]:
    | '(' b=genexp_internal ')' { b }
    | invalid_comprehension

# For optional call.
question_genexp[ast.GeneratorExp]:
    | '?(' b=genexp_internal ')' { b }

dictcomp[ast.DictComp]:
    | '{' b=for_if_clauses 'yield' ~ a=kvpair '}' { make_dictcomp(key=a[0], value=a[1], generators=b, LOCATIONS) }
    | invalid_dict_comprehension

if_comp_internal[ast.expr]:
    | 'if' '(' b=pipe_operator ')' NEWLINE* a=expression c=if_comp_elif {
        if_comp_exp(body=a, test=b, orelse=c, LOCATIONS)
     }
    | 'if' '(' b=pipe_operator ')' NEWLINE* a=expression c=[if_comp_else] {
        if_comp_exp(body=a, test=b, orelse=c, LOCATIONS)
     }
    | 'if' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=expression c=if_comp_elif {
        make_if_let_comp(pat_sub, cond, b, c, LOCATIONS)
     }
    | 'if' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=expression c=[if_comp_else] {
        make_if_let_comp(pat_sub, cond, b, c, LOCATIONS)
     }

if_comp_elif[ast.expr]:
    | 'elif' '(' b=pipe_operator ')' NEWLINE* a=expression c=if_comp_elif {
        if_comp_exp(body=a, test=b, orelse=c, LOCATIONS)
     }
    | 'elif' '(' b=pipe_operator ')' NEWLINE* a=expression c=[if_comp_else] {
        if_comp_exp(body=a, test=b, orelse=c, LOCATIONS)
     }
    | 'elif' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=expression c=if_comp_elif {
        make_if_let_comp(pat_sub, cond, b, c, LOCATIONS)
     }
    | 'elif' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* b=expression c=[if_comp_else] {
        make_if_let_comp(pat_sub, cond, b, c, LOCATIONS)
     }

if_comp_else[ast.expr]:
    | 'else' NEWLINE* a=expression { a }

if_comp_exp[ast.expr]:
    | '(' b=if_comp_internal ')' { b }

question_if_comp_exp[ast.expr]:
    | '?(' b=if_comp_internal ')' { b }

with_comp_internal[ast.expr]:
    | is_async=['async'] 'with' '(' a=';'.with_item+ [';'] ')' NEWLINE* b=expression {
        make_with_comp(is_async=is_async is not None, items=sum(a, []), body=b, LOCATIONS)
     }

with_comp[ast.expr]:
    | '(' b=with_comp_internal ')' { b }

question_with_comp[ast.expr]:
    | '?(' b=with_comp_internal ')' { b }

try_comp_internal[ast.expr]:
    |'try' NEWLINE* b=expression ex=try_comp_except* {
        make_try_comp(b, ex, LOCATIONS)
    }

try_comp_except[ast.Name]:
    | 'except' '(' e=expression t=['as' z=NAME { z.string }] ')' NEWLINE* b=expression {
        make_try_comp_except(t, e, b, LOCATIONS) }
    | 'except' '*' '(' e=expression t=['as' z=NAME { z.string }] ')' NEWLINE* b=expression {
        make_try_comp_except(t, e, b, LOCATIONS) }
    | 'except' NEWLINE* b=expression { make_try_comp_except(None, None, b, LOCATIONS) }

try_comp[ast.expr]:
    | '(' b=try_comp_internal ')' { b }

question_try_comp[ast.expr]:
    | '?(' b=try_comp_internal ')' { b }

match_comp_internal[ast.expr]:
    | 'match' '(' subject=subject_expr ')' NEWLINE* cases=match_comp_case+ {
        make_match_comp(subject, cases, LOCATIONS)
    }

match_comp[ast.expr]:
    | '(' b=match_comp_internal ')' { b }

question_match_comp[ast.expr]:
    | '?(' b=match_comp_internal ')' { b }

match_comp_case[ast.Name]:
    | 'case' '(' pattern=patterns ')' NEWLINE* guard=guard? NEWLINE* body=expression {
        make_match_comp_case(pattern, guard, body, LOCATIONS)
    }

while_comp_internal[ast.expr]:
    | 'while' '(' a=named_expression ')' NEWLINE* 'yield' b=expression {
        make_while_comp(a, b, LOCATIONS)
     }
    | 'while' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* 'yield' b=expression {
        make_while_let_comp(pat_sub, cond, b, LOCATIONS)
     }

while_comp[ast.expr]:
    | '(' b=while_comp_internal ')' { b }

question_while_comp[ast.expr]:
    | '?(' b=while_comp_internal ')' { b }

let_comp_internal[ast.expr]:
    | 'let' a=','.declaration_single_assignment+ ';' b=expression {
        make_let_comp(a, b, LOCATIONS)
     }

let_comp[ast.expr]:
    | '(' b=let_comp_internal ')' { b }

question_let_comp[ast.expr]:
    | '?(' b=let_comp_internal ')' { b }

comprehension_paren:
    | genexp
    | if_comp_exp
    | with_comp
    | try_comp
    | match_comp
    | while_comp
    | let_comp

question_comprehension_paren:
    | question_genexp
    | question_if_comp_exp
    | question_with_comp
    | question_try_comp
    | question_match_comp
    | question_while_comp
    | question_let_comp


# FUNCTION CALL ARGUMENTS
# =======================

arguments[Tuple[List[ast.expr], List[ast.keyword]]](memo):
    | a=args [','] &')' { a }
    | invalid_arguments

args[Tuple[List[ast.expr], List[ast.keyword]]]:
    | a=','.(starred_expression | ( assignment_expression | expression !':=') !'=')+ b=[',' k=kwargs {k}] {
        (a + ([e for e in b if isinstance(e, ast.Starred)] if b else []),
         ([e for e in b if not isinstance(e, ast.Starred)] if b else [])
        )
     }
    | a=kwargs {
        ([e for e in a if isinstance(e, ast.Starred)],
         [e for e in a if not isinstance(e, ast.Starred)])
    }

kwargs[List[ast.keyword | ast.Starred]]:
    | a=','.kwarg_or_starred+ ',' b=','.kwarg_or_double_starred+ { a + b }
    | ','.kwarg_or_starred+
    | ','.kwarg_or_double_starred+

starred_expression[ast.Starred]:
    | invalid_starred_expression
    | '*' a=expression { ast.Starred(value=a, ctx=Load, LOCATIONS) }
    | '*' { self.raise_syntax_error("Invalid star expression") }

kwarg_or_starred[ast.keyword | ast.expr]:
    | invalid_kwarg
    | a=NAME '=' b=expression { ast.keyword(arg=a.string, value=b, LOCATIONS) }
    | a=starred_expression { a }

kwarg_or_double_starred[ast.keyword]:
    | invalid_kwarg
    | a=NAME '=' b=expression { ast.keyword(arg=a.string, value=b, LOCATIONS) }   # XXX Unreachable
    | '**' a=expression { ast.keyword(arg=None, value=a, LOCATIONS) }

# ASSIGNMENT TARGETS
# ==================

# Generic targets
# ---------------

# NOTE: star_targets may contain *bitwise_or, targets may not.
# Assignment target. Does not suppoert optional access.
star_targets[ast.Tuple]:
    | a=star_target !',' { a }
    | a=star_target b=(',' c=star_target { c })* [','] {
        ast.Tuple(elts=[a] + b, ctx=Store, LOCATIONS)
     }

star_targets_list_seq[List[ast.expr]]: a=','.star_target_may_annot+ [','] { a }

star_targets_tuple_seq[List[ast.expr]]:
    | a=star_target_may_annot b=(',' c=star_target_may_annot { c })+ [','] { [a] + b }
    | a=star_target_may_annot ',' { [a] }

star_target[ast.Starred | ast.Name | ast.Tuple | ast.List | ast.Attribute | ast.Subscript](memo):
    | '*' a=(!'*' star_target) {
        ast.Starred(value=self.set_expr_context(a, Store), ctx=Store, LOCATIONS)
     }
    | target_with_star_atom

star_target_may_annot[ast.Name | ast.Tuple | ast.List | ast.Attribute | ast.Subscript]:
    | a=star_target b=annotation? { set_type_annotation(a, b) }

target_with_star_atom[ast.Name | ast.Tuple | ast.List | ast.Attribute | ast.Subscript](memo):
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(value=a, attr=b.string, ctx=Store, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(value=a, slice=b, ctx=Store, LOCATIONS) }
    | star_atom

star_atom[ast.Name| ast.Tuple | ast.List | ast.Attribute | ast.Subscript]:
    | a=NAME b=annotation? { set_type_annotation(ast.Name(id=a.string, ctx=Store, LOCATIONS), b) }
    | '(' a=target_with_star_atom ')' { self.set_expr_context(a, Store) }
    | '(' a=[star_targets_tuple_seq] ')' { ast.Tuple(elts=a, ctx=Store, LOCATIONS) }
    | '[' a=[star_targets_list_seq] ']' {  ast.List(elts=a, ctx=Store, LOCATIONS) }

single_target:
    | single_subscript_attribute_target
    | a=NAME { ast.Name(id=a.string, ctx=Store, LOCATIONS) }
    | '(' a=single_target ')' { a }

single_subscript_attribute_target:
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(value=a, attr=b.string, ctx=Store, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(value=a, slice=b, ctx=Store, LOCATIONS) }

# Assignment target primary.
t_primary:
    | a=t_primary '.' b=NAME &t_lookahead { ast.Attribute(value=a, attr=b.string, ctx=Load, LOCATIONS) }
    | a=t_primary '[' b=slices ']' &t_lookahead { ast.Subscript(value=a, slice=b, ctx=Load, LOCATIONS) }
    | a=t_primary b=genexp &t_lookahead { ast.Call(func=a, args=[b], keywords=[], LOCATIONS) }
    | a=t_primary '(' b=[arguments] ')' &t_lookahead {
        ast.Call(
            func=a,
            args=b[0] if b else [],
            keywords=b[1] if b else [],
            LOCATIONS,
        )
     }
    | a=atom &t_lookahead { a }

t_lookahead: '(' | '[' | '.'

# Targets for del statements
# --------------------------

del_targets: a=','.del_target+ [','] { a }

del_target (memo):
    | a=t_primary '.' b=NAME !t_lookahead { ast.Attribute(value=a, attr=b.string, ctx=Del, LOCATIONS) }
    | a=t_primary '[' b=slices ']' !t_lookahead { ast.Subscript(value=a, slice=b, ctx=Del, LOCATIONS) }
    | del_t_atom

del_t_atom:
    | a=NAME { ast.Name(id=a.string, ctx=Del, LOCATIONS) }
    | '(' a=del_target ')' { self.set_expr_context(a, Del) }
    | '(' a=[del_targets] ')' { ast.Tuple(elts=a, ctx=Del, LOCATIONS) }
    | '[' a=[del_targets] ']' { ast.List(elts=a, ctx=Del, LOCATIONS) }


# Declaration targets (irrefutable unpack special case)
# -----------------------------------------------------

decl_star_target[ast.Starred | ast.Name | ast.Tuple | ast.List](memo):
    | '*' a=(!'*' decl_star_target) {
        ast.Starred(value=self.set_expr_context(a, Store), ctx=Store, LOCATIONS)
     }
    | decl_star_atom

decl_star_atom[ast.Name | ast.Tuple | ast.List]:
    | a=NAME { ast.Name(id=a.string, ctx=Store, LOCATIONS) }

decl_star_target_may_annot[ast.expr]:
    | a=decl_star_target b=annotation? { set_type_annotation(a, b) }


# TYPING ELEMENTS
# ---------------

# type_expressions allow */** but ignore them
type_expressions[List[ast.expr]]:
    | a=','.expression+ ',' '*' b=expression ',' '**' c=expression { a + [b, c] }
    | a=','.expression+ ',' '*' b=expression { a + [b] }
    | a=','.expression+ ',' '**' b=expression { a + [b] }
    | '*' a=expression ',' '**' b=expression { [a, b] }
    | '*' a=expression { [a] }
    | '**' a=expression { [a] }
    | a=','.expression+ {a}

func_type_comment:
    | NEWLINE t=TYPE_COMMENT &(NEWLINE INDENT) { t.string }  # Must be followed by indented block
    | invalid_double_type_comments
    | TYPE_COMMENT

# ========================= END OF THE GRAMMAR ===========================



# ========================= START OF INVALID RULES =======================

# From here on, there are rules for invalid syntax with specialised error messages
invalid_arguments[NoReturn]:
    | ((','.(starred_expression | ( assignment_expression | expression !':=') !'=')+ ',' kwargs) | kwargs) a=',' ','.(starred_expression !'=')+ {
        self.raise_syntax_error_starting_from(
            "iterable argument unpacking follows keyword argument unpacking",
            a,
        )
     }
    | a=expression b=for_if_clauses ',' [args | expression for_if_clauses] {
        self.raise_syntax_error_known_range(
            "Generator expression must be parenthesized",
            a,
            (b[-1].ifs[-1] if b[-1].ifs else b[-1].iter)
        )
     }
    | a=NAME b='=' expression for_if_clauses {
        self.raise_syntax_error_known_range(
            "invalid syntax. Maybe you meant '==' or ':=' instead of '='?", a, b
        )
     }
    | (args ',')? a=NAME b='=' &(',' | ')') {
        self.raise_syntax_error_known_range("expected argument value expression", a, b)
     }
    | a=args b=for_if_clauses {
        self.raise_syntax_error_known_range(
            "Generator expression must be parenthesized",
            a[0][-1],
            (b[-1].ifs[-1] if b[-1].ifs else b[-1].iter),
        ) if len(a[0]) > 1 else None
     }
    | args ',' a=expression b=for_if_clauses {
        self.raise_syntax_error_known_range(
            "Generator expression must be parenthesized",
            a,
            (b[-1].ifs[-1] if b[-1].ifs else b[-1].iter),
        )
     }
    | a=args ',' args {
        self.raise_syntax_error(
            "positional argument follows keyword argument unpacking"
            if a[1][-1].arg is None else
            "positional argument follows keyword argument",
        )
     }
invalid_kwarg[NoReturn]:
    | a=('True'|'False'|'None') b='=' {
        self.raise_syntax_error_known_range(f"cannot assign to {a.string}", a, b)
     }
    | a=NAME b='=' expression for_if_clauses {
        self.raise_syntax_error_known_range(
            "invalid syntax. Maybe you meant '==' or ':=' instead of '='?", a, b
        )
     }
    | !(NAME '=') a=expression b='=' {
        self.raise_syntax_error_known_range(
            "expression cannot contain assignment, perhaps you meant \"==\"?", a, b,
        )
     }
    | a='**' expression '=' b=expression {
        self.raise_syntax_error_known_range(
            "cannot assign to keyword argument unpacking", a, b
        )
     }

# IMPORTANT: Note that the "_without_invalid" suffix causes the rule to
# not call invalid rules under it
expression_without_invalid[ast.AST]:
    | pipe_operator

invalid_expression[NoReturn]:
    # !(NAME STRING) is not matched so we don't show this error with some invalid string prefixes like: kf"dsfsdf"
    # Soft keywords need to also be ignored because they can be parsed as NAME NAME
    | !(NAME STRING | SOFT_KEYWORD) a=pipe_operator b=expression_without_invalid {
        (
            self.raise_syntax_error_known_range("invalid syntax. Perhaps you forgot a comma?", a, b)
            if not isinstance(a, ast.Name) or a.id not in ("print", "exec")
            else None
        )
     }

invalid_named_expression[NoReturn]:
    | a=expression ':=' expression {
        self.raise_syntax_error_known_location(
            f"cannot use assignment expressions with {self.get_expr_name(a)}", a
        )
     }
    # Use in_raw_rule
    | a=NAME '=' b=bitwise_or !('='|':=') {
        (
            None
            if self.in_recursive_rule else
            self.raise_syntax_error_known_range(
                "invalid syntax. Maybe you meant '==' or ':=' instead of '='?", a, b
            )
        )
     }
    | !(list|tuple|genexp|'True'|'None'|'False') a=bitwise_or b='=' bitwise_or !('='|':=') {
        (
            None
            if self.in_recursive_rule else
            self.raise_syntax_error_known_location(
                f"cannot assign to {self.get_expr_name(a)} here. Maybe you meant '==' instead of '='?", a
            )
        )
     }

invalid_assignment[NoReturn]:
    | a=invalid_ann_assign_target ':' expression {
        self.raise_syntax_error_known_location(
            f"only single target (not {self.get_expr_name(a)}) can be annotated", a
        )
     }
    | a=star_named_expression ',' star_named_expressions* ':' expression {
        self.raise_syntax_error_known_location("only single target (not tuple) can be annotated", a) }
    | a=expression ':' expression {
        self.raise_syntax_error_known_location("illegal target for annotation", a) }
    | (star_targets '=')* a=star_expressions '=' {
        self.raise_syntax_error_invalid_target(Target.STAR_TARGETS, a)
     }
    | (star_targets '=')* a=yield_expr '=' {
        self.raise_syntax_error_known_location("assignment to yield expression not possible", a)
     }
    | a=star_expressions augassign (yield_expr | star_expressions) {
        self.raise_syntax_error_known_location(
            f"'{self.get_expr_name(a)}' is an illegal expression for augmented assignment", a
        )
     }
invalid_ann_assign_target[ast.AST]:
    | a=list { a }
    | a=tuple { a }
    | '(' a=invalid_ann_assign_target ')' { a }
invalid_del_stmt[NoReturn]:
    | 'del' a=star_expressions {
        self.raise_syntax_error_invalid_target(Target.DEL_TARGETS, a)
     }
invalid_block[NoReturn]:
    | NEWLINE !INDENT { self.raise_indentation_error("expected an indented block") }
invalid_comprehension[NoReturn]:
    | ('[' | '(' | '{') a=starred_expression for_if_clauses {
        self.raise_syntax_error_known_location("iterable unpacking cannot be used in comprehension", a)
     }
    | ('[' | '{') a=star_named_expression ',' b=star_named_expressions for_if_clauses {
        self.raise_syntax_error_known_range(
            "did you forget parentheses around the comprehension target?", a, b[-1]
        )
     }
    | ('[' | '{') a=star_named_expression b=',' for_if_clauses {
        self.raise_syntax_error_known_range(
            "did you forget parentheses around the comprehension target?", a, b
        )
     }
invalid_dict_comprehension[NoReturn]:
    | '{' a='**' bitwise_or for_if_clauses '}' {
        self.raise_syntax_error_known_location("dict unpacking cannot be used in dict comprehension", a)
     }
invalid_parameters[NoReturn]:
    | a="/" ',' {
        self.raise_syntax_error_known_location("at least one argument must precede /", a)
     }
    | (slash_no_default | slash_with_default) param_maybe_default* a='/' {
        self.raise_syntax_error_known_location("/ may appear only once", a)
     }
    | slash_no_default? param_no_default* invalid_parameters_helper a=param_no_default {
        self.raise_syntax_error_known_location(
            "parameter without a default follows parameter with a default", a
        )
     }
    # | param_no_default* a='(' param_no_default+ ','? b=')' {
    #     self.raise_syntax_error_known_range(
    #         "Function parameters cannot be parenthesized", a, b
    #     )
    #  }
    | (slash_no_default | slash_with_default)? param_maybe_default* '*' (',' | param_no_default) param_maybe_default* a='/' {
        self.raise_syntax_error_known_location("/ must be ahead of *", a)
     }
    | param_maybe_default+ '/' a='*' {
        self.raise_syntax_error_known_location("expected comma between / and *", a)
     }
invalid_default:
    | a='=' &(')'|',') {
        self.raise_syntax_error_known_location("expected default value expression", a)
     }
invalid_star_etc:
    | a='*' (')' | ',' (')' | '**')) {
        self.raise_syntax_error_known_location("named arguments must follow bare *", a)
     }
    | '*' ',' TYPE_COMMENT { self.raise_syntax_error("bare * has associated type comment") }
    | '*' param a='=' {
        self.raise_syntax_error_known_location("var-positional argument cannot have default value", a)
     }
    | '*' (param_no_default | ',') param_maybe_default* a='*' (param_no_default | ',') {
        self.raise_syntax_error_known_location("* argument may appear only once", a)
     }
invalid_kwds:
    | '**' param a='=' {
        self.raise_syntax_error_known_location("var-keyword argument cannot have default value", a)
     }
    | '**' param ',' a=param {
        self.raise_syntax_error_known_location("arguments cannot follow var-keyword argument", a)
     }
    | '**' param ',' a=('*'|'**'|'/') {
        self.raise_syntax_error_known_location("arguments cannot follow var-keyword argument", a)
     }
invalid_parameters_helper: # This is only there to avoid type errors
    | a=slash_with_default { [a] }
    | a=param_with_default+

invalid_double_type_comments[NoReturn]:
    | TYPE_COMMENT NEWLINE TYPE_COMMENT NEWLINE INDENT {
        self.raise_syntax_error("Cannot have two type comments on def")
     }
invalid_with_item[NoReturn]:
    | expression 'as' a=expression &(',' | ')' | ':') {
        self.raise_syntax_error_invalid_target(Target.STAR_TARGETS, a)
     }

invalid_for_target[NoReturn]:
    | 'async'? 'for' a=star_expressions {
        self.raise_syntax_error_invalid_target(Target.FOR_TARGETS, a)
     }

invalid_group[NoReturn]:
    | '(' a=starred_expression ')' {
        self.raise_syntax_error_known_location("cannot use starred expression here", a)
     }
    | '(' a='**' expression ')' {
        self.raise_syntax_error_known_location("cannot use double starred expression here", a)
     }
invalid_import:
    | a='import' ','.dotted_name+ 'from' dotted_name {
        self.raise_syntax_error_starting_from(
            "Did you mean to use 'from ... import ...' instead?", a
        )
     }
invalid_import_from_targets[NoReturn]:
    | import_from_as_names ',' NEWLINE {
        self.raise_syntax_error("trailing comma not allowed without surrounding parentheses")
     }

invalid_with_stmt[None]:
    | ['async'] 'with' !('(' | decl_keyword) { self.raise_syntax_error("expected '('") }
    | ['async'] 'with' '(' a=with_item ')' NEWLINE* !'{' {
        self.raise_syntax_error("expected '{'")
    }

invalid_try_stmt[NoReturn]:
    | 'try' !'{' {
        self.raise_syntax_error("expected '{'")
     }
    | 'try' block !('except' | 'finally') {
        self.raise_syntax_error("expected 'except' or 'finally' block")
     }
    | 'try' block* except_block+ a='except' b='*' {
        self.raise_syntax_error_known_range(
            "cannot have both 'except' and 'except*' on the same 'try'", a, b
        )
     }
    | 'try' block* except_star_block+ a='except' {
        self.raise_syntax_error_known_location(
            "cannot have both 'except' and 'except*' on the same 'try'", a
        )
     }
invalid_except_stmt[None]:
    | 'except' '*'? '(' a=expression ',' expressions ['as' NAME ] ')' {
        self.raise_syntax_error_starting_from("multiple exception types must be parenthesized", a)
     }
    | a='except' '*'? '(' expression ['as' NAME ] ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
    | a='except' '*'? !('{'|'(') { self.raise_syntax_error("expected '('") }
    | a='except' '*' NEWLINE* '{' {
        self.raise_syntax_error("expected one or more exception types")
     }
invalid_finally_stmt[NoReturn]:
    | a='finally' !'{' {
        self.raise_syntax_error("expected '{'")
     }
invalid_match_stmt[NoReturn]:
    | "match" !'(' { self.raise_syntax_error("expected '('") }
    | "match" '(' subject=subject_expr ')' NEWLINE* !'{' {
        self.raise_syntax_error("expected '{'")
     }
invalid_case_block[NoReturn]:
    | "case" !'(' { self.raise_syntax_error("expected '('") }
    | "case" '(' pattern=patterns ')' NEWLINE* guard=guard? NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
invalid_as_pattern[NoReturn]:
    | or_pattern 'as' a="_" {
        self.raise_syntax_error_known_location("cannot use '_' as a target of as pattern", a)
     }
    | or_pattern 'as' !NAME {
        self.raise_syntax_error("invalid as pattern target")
     }
invalid_class_pattern[NoReturn]:
    | name_or_attr '(' a=invalid_class_argument_pattern  {
        self.raise_syntax_error_known_range(
            "positional patterns follow keyword patterns", a[0], a[-1]
        )
     }
invalid_class_argument_pattern[List[ast.pattern]]:
    | [positional_patterns ','] keyword_patterns ',' a=positional_patterns { a }

invalid_if_stmt[NoReturn]:
    | 'if' !'(' { self.raise_syntax_error("expected '('") }
    | 'if' '(' a=named_expression ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
    | 'if' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }

invalid_elif_stmt[NoReturn]:
    | 'elif' !'(' { self.raise_syntax_error("expected '('") }
    | 'elif' '(' a=named_expression ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
    | 'elif' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }

invalid_else_stmt[NoReturn]:
    | a='else' NEWLINE * !'{' { self.raise_syntax_error("expected '{'") }

invalid_while_stmt[NoReturn]:
    | 'while' !'(' { self.raise_syntax_error("expected '('") }
    | 'while' '(' a=named_expression ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
    # while-let statement as pattern matching
    | 'while' '(' 'let' pat_sub=','.pattern_assignment+ cond=[';' a=named_expression {a}] ')' NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }

invalid_for_stmt[NoReturn]:
    | ['async'] 'for' !'(' { self.raise_syntax_error("expected '('") }

invalid_def_raw[NoReturn]:
    | ['async'] a='def' NAME  [type_params] '(' [params] ')' '->' expression NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }
    | ['async'] a='def' NAME  [type_params] '(' [params] ')' ':' { self.raise_syntax_error("use '->' for function return type") }
    | ['async'] a='def' NAME  [type_params] '(' [params] ')' !('{' | '->') { self.raise_syntax_error("expected '{'") }

invalid_class_def_raw[NoReturn]:
    | 'class' NAME [type_params] ['(' [arguments] ')'] NEWLINE* !'{' { self.raise_syntax_error("expected '{'") }

invalid_double_starred_kvpairs[None]:
    | ','.double_starred_kvpair+ ',' invalid_kvpair
    | expression ':' a='*' bitwise_or {
        self.raise_syntax_error_starting_from("cannot use a starred expression in a dictionary value", a)
     }
    | expression a=':' &('}'|',') {
        self.raise_syntax_error_known_location("expression expected after dictionary key and ':'", a)
     }
invalid_kvpair[None]:
    | a=expression !(':') {
        self.raise_raw_syntax_error(
            "':' expected after dictionary key",
            (a.lineno, a.col_offset),
            (a.end_lineno, a.end_col_offset)
        )
     }
    | expression ':' a='*' bitwise_or {
        self.raise_syntax_error_starting_from("cannot use a starred expression in a dictionary value", a)
     }
    | expression a=':' &('}'|',') {
        self.raise_syntax_error_known_location(
            "expression expected after dictionary key and ':'", a
        )
     }
    | expression a=':' {
        self.raise_syntax_error_known_location("expression expected after dictionary key and ':'", a)
     }
invalid_starred_expression:
    | a='*' expression '=' b=expression {
        self.raise_syntax_error_known_range(
            "cannot assign to iterable argument unpacking", a, b
        )
     }
invalid_replacement_field:
    | '{' a='=' { self.raise_syntax_error_known_location("f-string: valid expression required before '='", a) }
    | '{' a='!' { self.raise_syntax_error_known_location("f-string: valid expression required before '!'", a) }
    | '{' a=':' { self.raise_syntax_error_known_location("f-string: valid expression required before ':'", a) }
    | '{' a='}' { self.raise_syntax_error_known_location("f-string: valid expression required before '}'", a) }
    | '{' !(yield_expr | star_expressions) {
        self.raise_syntax_error_on_next_token(
            "f-string: expecting a valid expression after '{'"
        )
     }
    | '{' (yield_expr | star_expressions) !('=' | '!' | ':' | '}') {
        self.raise_syntax_error_on_next_token("f-string: expecting '=', or '!', or ':', or '}'") }
    | '{' (yield_expr | star_expressions) '=' !('!' | ':' | '}') {
       self.raise_syntax_error_on_next_token("f-string: expecting '!', or ':', or '}'")
     }
    | '{' (yield_expr | star_expressions) '='? invalid_conversion_character
    | '{' (yield_expr | star_expressions) '='? ['!' NAME] !(':' | '}') {
        self.raise_syntax_error_on_next_token("f-string: expecting ':' or '}'")
     }
    | '{' (yield_expr | star_expressions) '='? ['!' NAME] ':' fstring_format_spec* !'}' {
        self.raise_syntax_error_on_next_token("f-string: expecting '}', or format specs")
     }
    | '{' (yield_expr | star_expressions) '='? ['!' NAME] !'}' {
        self.raise_syntax_error_on_next_token("f-string: expecting '}'")
     }

invalid_conversion_character:
    | '!' &(':' | '}') { self.raise_syntax_error_on_next_token("f-string: missing conversion character") }
    | '!' !NAME { self.raise_syntax_error_on_next_token("f-string: invalid conversion character") }
